{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e3b272e",
   "metadata": {
    "id": "1e3b272e"
   },
   "source": [
    "# Grad Project \\# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f80a6e1",
   "metadata": {
    "id": "1f80a6e1"
   },
   "source": [
    "## Imports and Utilities\n",
    "\n",
    "These are import and utility functions, and also scaffolding of functions that we have provided for you for the project.\n",
    "\n",
    "Read through the code in the other .py files. You will need to understand the functions in those files to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87555ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebbf318",
   "metadata": {
    "id": "aebbf318"
   },
   "outputs": [],
   "source": [
    "from typing import (Iterable, Dict, Optional)\n",
    "# Setup matplotlib animation\n",
    "import matplotlib\n",
    "from IPython.display import HTML\n",
    "matplotlib.rc('animation', html='jshtml')\n",
    "import random\n",
    "import dataclasses\n",
    "import numpy as np\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977cb3fb-adb8-4de6-8354-199a8ced9d0a",
   "metadata": {},
   "source": [
    "Throughout this course, we will be developing a \"search and rescue\" robot who will be charged with navigating a sometimes dangerous grid to find and help people in need. We will first consider planning to navigate to, pick up, and drop off people at a hospital. You will use some of the algorithms we have already discussed in this subject, including heuristics search and Monte-Carlo tree search. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacef706-d192-48b2-b264-baaa51f518a9",
   "metadata": {},
   "source": [
    "# 1. Pickup Agent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf878e67-4161-48a5-b04e-e7b86ea86252",
   "metadata": {},
   "source": [
    "Let us first consider a deterministic gridworld-like domain that consists of a robot, a patient and a target hospital.\n",
    "\n",
    "![Pickup figure](just_wait.png)\n",
    "\n",
    "The robot's goal is to pick up the patient and rescue them to the target hospital. As you can see in the above figure, the blue circle represents our robot, the orange circle represents the patient, and the red cross represents the hospital. The environment has one-way roadblocks, indicated by a flat triangle between two cells. For example, the agent can only move down from (1,1) to  (2,1) but not up. The environment also has two-way roadblocks, indicated by thick black boundaries between two cells. For example, the figure above has a long corridor formed by two walls that the agent can travel down from its initial position, but not sideways.\n",
    "\n",
    "In the following code block, we have provided an implementation of the `PickupProblem`, in terms of states, actions and costs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280966e-8c0d-457e-9166-5f7bc1ad68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickup_problem import OneWayBlock, PickupProblem, PickupProblemState\n",
    "### SET UP THE SIMPLE MAZE \n",
    "\n",
    "def add_wall(cell_1, cell_2, one_ways_list):\n",
    "    one_ways_list.append(OneWayBlock(cell_1, cell_2))\n",
    "    one_ways_list.append(OneWayBlock(cell_2, cell_1))\n",
    "\n",
    "simple_one_ways = []\n",
    "for i in range(7):\n",
    "    simple_one_ways.append(OneWayBlock((i+1, 1), (i,1)))\n",
    "    simple_one_ways.append(OneWayBlock((i, 3), (i+1,3)))\n",
    "\n",
    "for i in range(1,7):\n",
    "    add_wall((i,1), (i, 0), simple_one_ways)\n",
    "    add_wall((i,1), (i, 2), simple_one_ways)\n",
    "    # simple_one_ways.append(OneWayBlock((i, 1), (i, 0)))\n",
    "    # simple_one_ways.append(OneWayBlock((i, 1), (i, 2)))\n",
    "    \n",
    "add_wall((0, 0), (1, 0), simple_one_ways)\n",
    "add_wall((7, 0), (6, 0), simple_one_ways)\n",
    "add_wall((0, 2), (1, 2), simple_one_ways)\n",
    "add_wall((7, 2), (6, 2), simple_one_ways)\n",
    "\n",
    "SimpleProblem = PickupProblem((8, 4), (0, 1), (7, 0), (0, 3), simple_one_ways)\n",
    "\n",
    "### END OF SET UP THE SIMPLE MAZE \n",
    "SimpleProblem.render(SimpleProblem.initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bacf74-02c7-415c-b518-557494c5f1c8",
   "metadata": {},
   "source": [
    "## 1.1 A* In Pickup \n",
    "\n",
    "Please implement A* in the context of the Pickup domain using the (not very good) default heuristic that is included in the `PathCostProblem` definition. In the helper code above, we gave you a method `run_best_first_search` that takes a `PathCostProblem` as an argument. That method might be helpful to you here. \n",
    "\n",
    "For reference, our solution is **2** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481b3eb1-3a1c-4513-a11c-3170ac59e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_astar_search(problem: PathCostProblem, step_budget: int = 10000):\n",
    "    \"\"\"A* search.\n",
    "\n",
    "    Use the implementation of `run_best_first_search` with the default heuristic.\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplementedError() \n",
    "    return path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd298a0-e356-4b4b-9c63-9342448c3ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = run_astar_search(SimpleProblem)\n",
    "print(\"Path information:\")\n",
    "print(\"num_steps:\", search_result[3])\n",
    "print(\"path costs:\",search_result[2],\"total cost:\",sum(search_result[2]))\n",
    "print(\"actions found is \", search_result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d62eb-4f65-48ad-b93e-92815c355430",
   "metadata": {},
   "source": [
    "## 1.2 Better heuristics for Pickup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39158e6f-1336-4cc3-8a96-6c6a406aa26d",
   "metadata": {},
   "source": [
    "Please provide a better heuristic, and show that it outperforms the original heuristic. \n",
    "\n",
    "We have not defined in this question what it means to \"outperform\" here, so this is a free-form answer. You are welcome to add extra instrumentation to any of the classes to help with analysis, although our solution does not require that.  \n",
    "\n",
    "For reference, our heuristic is **12** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b38803-27db-4c2d-94d6-7eb40089a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_astar_search_faster(problem: PathCostProblem, step_budget: int = 10000):\n",
    "    \"\"\"A* search.\n",
    "    Write a better heuristic than the default provided one\n",
    "    Use your heuristic implementation with `run_best_first_search`.\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplementedError() \n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4cccdb-e8e8-4f8c-a943-6f7927c15311",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = run_astar_search_faster(SimpleProblem)\n",
    "# can print evidence of improvement below\n",
    "# eg print(...)print(\"Path information:\")\n",
    "print(\"num_steps:\", search_result[3])\n",
    "print(\"path costs:\",search_result[2],\"total cost:\",sum(search_result[2]))\n",
    "print(\"actions found is \", search_result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb87712-29f2-4e96-8647-cbafeea7729b",
   "metadata": {},
   "source": [
    "## Answer to Question 1.2\n",
    "**Describe how you have improved the heuristic and how you have shown that it outperforms the original heuristic.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dc3339-ab4a-44ed-8c6b-69db2a283776",
   "metadata": {},
   "source": [
    "# 2. The evolution of fire \n",
    "\n",
    "The above problem is a deterministic search problem. But let's make the problem more interesting: Our grid now has fire. The fire starts at some initial locations (represented by red grid cells in the first figure) and evolves through time.\n",
    "\n",
    "Thanks to our research at MIT, we know the exact model of how fire evolves through time.\n",
    "\n",
    "Most notably, the fire grid at time $t$ is independent of the robot and patient, and depends only on the fire grid at the previous time step. Fire also completely ignores walls, roadblocks and the hospital.\n",
    "\n",
    "Further, given the fire grid at time $t$, the probabilities of fire at any two different cells at time $t+1$ are independent: \n",
    "$$\n",
    "P(\\mathbf{F}^{t+1} \\mid \\mathbf{F}^t) = \\prod_{(i, j) \\in \\mathtt{grid}} P\\left(\\mathbf{F}_{(i, j)}^{t+1} \\mid \\mathbf{F}_{(i', j') \\in \\mathtt{neighbors}((i, j))}^t\\right),\n",
    "$$ where $\\mathtt{neighbors}((i, j)) = \\{ (i', j') \\mid  |i - i'| \\le 1 \\land |j - j'| \\le 1 \\land (i, j) \\neq (i', j') \\}$ is the 3 by 3 patch of cells centered at $(i, j)$, including $(i, j)$.\n",
    "Further, at time $t + 1$, the probability of fire in cell $(i, j)$ is the weighted \n",
    "probability of its neighboring cells on fire at time $t$ for a given fixed weight matrix\n",
    "$W \\in \\mathbb{R}^{3\\times 3}$:\n",
    "$$\n",
    "P\\left(\\mathbf{F}_{(i, j)}^{t+1} \\mid \\mathbf{F}_{(i', j') \\in \\mathtt{neighbors}((i, j))}^t\\right) \\propto \\sum_{i', j'} W[i' - i + 1, j' - j + 1] \\cdot F[i', j'] .\n",
    "$$\n",
    "\n",
    "You might recognize that given the fire grid at time $t$, a matrix of 0-1 values, \n",
    "the probability of fire at time $t+1$ is the [2D convolution](http://www.songho.ca/dsp/convolution/convolution2d_example.html) of the fire grid and the weights $W$ (normalized such that the entries sum to one).\n",
    "\n",
    "Here's another way to understand the fire process:\n",
    "- We start with some initial fire grid. \n",
    "- At each time step $t$, for each cell, we randomly select a neighbor (including the current cell) with probability proportional to the weights matrix; then, the current cell at time $t+1$ gets the selected neighbor's fire value at time $t$. Note that neighors outside of the grid does not have fire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92450f37-c01b-4e1e-b20f-414475f85f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fire_process import (\n",
    "    FireMDP, \n",
    "    FireMDPState, \n",
    "    FireProcess,\n",
    "    get_problem,\n",
    ")  # read and understand this code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30de07b-7226-4b3b-8835-b5a4bebd0713",
   "metadata": {},
   "source": [
    "### Our Approach\n",
    "\n",
    "We are going to adopt an online-planning approach, where at every step, our agent:\n",
    "- plans according to the current state,\n",
    "- executes an action, and\n",
    "- observes a new state of the fire grid and replans (i.e., restarts from step one).\n",
    "\n",
    "We will consider two different styles of planning: determinized approximation and Monte-Carlo tree search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bba2f91-99b1-439f-9fe5-c47844cad784",
   "metadata": {},
   "source": [
    "#### Approximate, Determinize and Replan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6471f8e-2a61-4f92-8f6f-16fca44e2659",
   "metadata": {},
   "source": [
    "In our first approach, at each planning step, we will try to find an open-loop plan that is most likely to succeed.\n",
    "In particular, we turn the MDP into an min-cost path problem:\n",
    "- The state space no longer has the fire grid, but only contains the state of the pickup and rescue problem.\n",
    "- For a step $(s_t, a, s_{t+1})$ at time $t$, we charge a cost of $c - \\log \\left(1 - P\\left(\\mathtt{on\\_fire}_{t+1}(s_{t+1})\\right)\\right)$, where $c$ is a small cost for taking each step, and $P\\left(\\mathtt{on\\_fire}_t(s)\\right)$ is the marginal probability of stepping on fire at state $s$ at time $t$. \n",
    "- We try to find the least-cost path to reach the patient and rescue them to the hospital --- this path becomes our found open-loop plan.\n",
    "\n",
    "Once we have a min-cost path problem, we can use A* search with a simple heuristic that ignores fire.\n",
    "In particular, we will use a simple heuristic that is the sum of the manhattan distance from robot to the patient and the manhattan distance \n",
    "from patient to the hospital, scaled by the small cost `c` charged at each step. \n",
    "Note that when the robot is carrying a patient, the distance between the robot and the patient is zero.\n",
    "\n",
    "_Hints_: \n",
    "- Before you code, try to derive the marginal probabilities of each grid cell on fire at time $t$ as an expression of the marginal probabilities of each grid cell on fire at time $t-1$. What do you find? More concretely, you may start small: Consider a grid consisting of only two cells, named $X$ and $Y$, and assume that $W$ is uniform. Then, try to write the marginal probability of cell $P(X_t=1 | X_0, Y_0)$ as an expression of $P(X_{t-1}=1 | X_0, Y_0)$ and $P(Y_{t-1}=1 | X_0, Y_0)$.\n",
    "- We can formulate the A* state as $(\\mathtt{robot\\_loc}, \\text{carrying\\_patient}, \\mathtt{time})$ and use time as an index into a precomputed sequence of the marginal probabilities that each cell is on fire. See `DeterminizedFireMDPState` and `DeterminizedFireMDP` for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f547cb2",
   "metadata": {
    "id": "8f547cb2"
   },
   "source": [
    "## 2.1 Determinized Min-cost Path Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f26de4-e69f-4e71-af93-4569f2904611",
   "metadata": {},
   "source": [
    "Please complete the implementation of `DeterminizedFireMDP`. In particular, you should:\n",
    "- Complete the function `fire_dist_at_time` to compute the log-likelihood of each cell being on fire at time $t$ given the true fire state at time $0$. \n",
    "- Using your implementation of `fire_dist_at_time`, complete the function `step_cost`.\n",
    "- Complete the rest of the `DeterminizedFireMDP` and implement the heuristic function `h` based on description above. It might look remarkably similar to your heuristic from question 1.2, the only difference being that `DeterminizedFireMDP` contains a `PathCostProblem`. \n",
    "    \n",
    "\n",
    "For reference, our solution is **91** line(s) of code, including the code we have provided for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4197cb",
   "metadata": {
    "id": "9e4197cb"
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True, eq=True, order=True)\n",
    "class DeterminizedFireMDPState(PickupProblemState):\n",
    "  \"\"\"A state for the DeterminizedFireMDP.\n",
    "    \n",
    "    The state is a pair of the PickupProblemState and a time step $t$.\n",
    "    \"\"\"\n",
    "  time: int = 0\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class DeterminizedFireMDP(PathCostProblem):\n",
    "    \"\"\"Determinized version of the fire MDP --- tries to find the solution path\n",
    "    that is most likely to succeed.\n",
    "    \"\"\"\n",
    "    pickup_problem: PickupProblem\n",
    "    fire_process: FireProcess\n",
    "\n",
    "    # Additional cost for each step.\n",
    "    # Can be 0 but we might have 0-cost arcs if the success probability is 1.\n",
    "    action_cost = 1e-6\n",
    "\n",
    "    # Use this to cache precomputed fire distributions, so we don't have to recompute them.\n",
    "    fire_dists_cache: Dict[int, np.ndarray] = dataclasses.field(\n",
    "        init=False,\n",
    "        default_factory=dict,\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert (self.pickup_problem.grid_shape ==\n",
    "                self.fire_process.initial_fire_grid.shape)\n",
    "\n",
    "    @property\n",
    "    def initial(self) -> DeterminizedFireMDPState:\n",
    "        return DeterminizedFireMDPState(\n",
    "            *dataclasses.astuple(self.pickup_problem.initial),\n",
    "            time=0,\n",
    "        )\n",
    "\n",
    "    def actions(self, state: DeterminizedFireMDPState) -> Iterable[Action]:\n",
    "        raise NotImplementedError() \n",
    "    \n",
    "    def step(self, state: DeterminizedFireMDPState,\n",
    "             action: Action) -> State:\n",
    "        \"\"\"We automatically pick up patient if we're on that square.\"\"\"\n",
    "        raise NotImplementedError() \n",
    "    \n",
    "    def goal_test(self, state: DeterminizedFireMDPState) -> bool:\n",
    "        \"\"\"True if at hospital and holding patient.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def step_cost(self, state1: DeterminizedFireMDPState, action: Action,\n",
    "                  state2: DeterminizedFireMDPState) -> float:\n",
    "        raise NotImplementedError() \n",
    "\n",
    "    def fire_dist_at_time(self, t: int) -> np.ndarray:\n",
    "        \"\"\"Return the marginal distribution of fire grid at time $t$. This should populate and use caching in self.fire_dists_cache\"\"\"\n",
    "\n",
    "        raise NotImplementedError() \n",
    "    \n",
    "    def h(self, state: DeterminizedFireMDPState) -> float:\n",
    "        \"\"\"heuristic based on the manhattan distance to the patient and hospital.\"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26622f4",
   "metadata": {
    "id": "a26622f4"
   },
   "source": [
    "## 2.2 Determinized Fire MDP Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af510b3f-619c-4b15-929a-3be58865c6ed",
   "metadata": {},
   "source": [
    "Please complete the implementation of FireMDPDeterminizedAStarAgent. Note that we have filled in most of the implementation for you --- including the call to `run_astar_search` from section 1. All you need to implement is the determinized_problem method.\n",
    "\n",
    "For reference, our solution is **36** line(s) of code, including the code we have provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f455356c",
   "metadata": {
    "id": "f455356c"
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class FireMDPDeterminizedAStarAgent(Agent):\n",
    "    \"\"\"Agent that uses A* to plan a path to the goal in a determinized\n",
    "    version of the problem. Does not need any internal state since we\n",
    "    re-determinize the problem at each step.\n",
    "    \"\"\"\n",
    "\n",
    "    problem: FireMDP\n",
    "    step_budget: int = 10000\n",
    "\n",
    "    def determinized_problem(self,\n",
    "                             state: FireMDPState) -> DeterminizedFireMDP:\n",
    "        \"\"\"Returns a determinized approximation of the fire MDP.\"\"\"\n",
    "        raise NotImplementedError() \n",
    "        \n",
    "    def act(self, state: FireMDPState) -> Action:\n",
    "        problem = self.determinized_problem(state)\n",
    "        try:\n",
    "            plan = run_astar_search(problem, self.step_budget)\n",
    "        except SearchFailed:\n",
    "            print(\"Search failed, performing a random action\")\n",
    "            return random.choice(list(self.problem.actions(state)))\n",
    "        return plan[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a65dbc6-dced-408a-a79a-914b606b090e",
   "metadata": {},
   "source": [
    "Now that we have the agent, let's try to experiment with it under some environments!\n",
    "\n",
    "We have provided you various environments under the `get_problem` function.\n",
    "Try to run your agent in each provided environment several times. \n",
    "You can visualize the agent's behavior using the `run_agent_on_problem` and `animate_trajectory` functions.\n",
    "\n",
    "Let's take a closer look at the particular MDP of `get_problem(\"just_wait\")`. \n",
    "You might ask yourself: what is your agent's behavior (on average)? \n",
    "In particular, does the robot \"wait\" by patrolling in the top row for a while, \n",
    "and then moves out to rescue the patient? \n",
    "If not, then it is very likely that your agent implementation is buggy! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911eb03d-da2e-4175-b3d7-f3cb9ad45bae",
   "metadata": {},
   "source": [
    "### Experiment 1 ####\n",
    "Please try to generate an animation of a **successful run** (i.e., the robot successfully rescues the patient) of the agent in the just_wait MDP, but **the fire does not completely die out when the robot moves down from the top row**. You might need to repeat the experiment a few times to produce this animation. If you failed to create this animation after a handful of trials (say, 15), your agent implementation might be buggy. Please feel free to reach out to us anytime if you get stuck.\n",
    "\n",
    "Submit the animation as **just_wait_determinized.mp4**. Videos in the jupyter notebook **are not supported**, so you will need to submit the video separately.\n",
    "\n",
    "You can view a video of the animated trajectory in the notebook by running the following code. Again, these videos will **not** be visible to the graders, so you will need to submit the video separately.\n",
    "```python\n",
    "HTML(animate_trajectory(...).to_html5_video())\n",
    "```\n",
    "To save the video to a file, you can use the following code:\n",
    "```python\n",
    "animate_trajectory(...).save(\"just_wait_determinized.mp4\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc241f6f-130a-42e5-8943-87564b5e8887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1e7119d-b487-4d98-b5ae-df126466a074",
   "metadata": {},
   "source": [
    "## 2.3 What's the Right Choice? \n",
    "Sometimes our determinized agent finds plan that is not optimal. To see the above effect, try to run the determinized agent in the MDP get_problem(\"the_choice\"). In this environment, the agent faces a choice of going right or down from the initial location.\n",
    "\n",
    "It may choose the shortcut by taking the down action. But, it risks itself getting close to the fire next to the one-way passage, and it cannot hide from the fire in this passage.\n",
    "It may also accept the challenge by taking the right action. Here, the robot moves to a large room with more fire than the one-way passage. But it can move around to try its best to avoid fire, until it finds a clear path to rescue the patient.\n",
    "\n",
    "#### Experiment 2 ###\n",
    "\n",
    "Similar to experiment 1, please visualize the behavior of the determinized planning agent in this environment, and generate an animation of a successful run. What choice does your determinized agent make?\n",
    "\n",
    "Submit the animation as **the_choice_determinized.mp4**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574a0c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6af43f7d",
   "metadata": {
    "id": "6af43f7d"
   },
   "source": [
    "# 3. MCTS Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d8364",
   "metadata": {
    "id": "c01d8364"
   },
   "source": [
    "Now that we have seen a failure mode of our determinized planning agent, let's try to do better with closed-loop planning with MCTS!\n",
    "\n",
    "With MCTS, we have more of a chance of hedging bets, so we might be inclined to go in directions where there are more options in case we get caught, even if the expected open-loop cost is higher.\n",
    "\n",
    "We have provided you with an MCTS implementation, `run_mcts_search`. Please take a look at the documentation of `run_mcts_search` to understand how to use it, then implement an MCTS agent for MDPs.\n",
    "\n",
    "Please complete the implementation of `MCTSAgent`.\n",
    "\n",
    "_Hint: You can pass in the `self.planning_horizon` to `run_mcts_search`,\n",
    "to handle both infinite-horizon problems (by receding-horizon planning) and finite-horizon problems._\n",
    "\n",
    "\n",
    "For reference, our solution is **42** line(s) of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6733b0",
   "metadata": {
    "id": "df6733b0"
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class MCTSAgent(Agent):\n",
    "    \"\"\"Agent that uses Monte Carlo Tree Search to plan a path to the goal.\n",
    "\n",
    "    The agent simply wraps `run_mcts_search`, and it should work for any MDP.\n",
    "    \"\"\"\n",
    "\n",
    "    problem: MDP\n",
    "\n",
    "    # An optional receding horizon to use for the planning\n",
    "    # If not provided, the problem must have a finite horizon\n",
    "    receding_horizon: Optional[int] = None\n",
    "\n",
    "    C: float = np.sqrt(2)\n",
    "    iteration_budget: int = 1000\n",
    "\n",
    "    t: int = dataclasses.field(default=0, init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.receding_horizon is None:\n",
    "            assert self.problem.horizon != np.inf\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "\n",
    "    @property\n",
    "    def planning_horizon(self) -> int:\n",
    "        \"\"\"Returns the planning horizon for the current time step.\"\"\"\n",
    "        if self.receding_horizon is None:\n",
    "            return self.problem.horizon - self.t\n",
    "        return self.receding_horizon\n",
    "\n",
    "    def act(self, state: State) -> Action:\n",
    "        \"\"\"Return the action to take at state.\"\"\"\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef1dd96-f42d-4a39-8d3d-3b4198e1cbbb",
   "metadata": {},
   "source": [
    "## 3.1 Making the Right Choice! ###\n",
    "Let's run our MCTS agent in `the_choice` MDP, and see if it makes the right choice!\n",
    "\n",
    "#### Experiment 3 ###\n",
    "Similar to experiment 2, please visualize the behavior of the MCTS planning agent in the_choice MDP and generate an animation of a successful run.\n",
    "\n",
    "Submit the animation as **the_choice_mcts.mp4**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e57ae7-803a-4f15-9d23-52d16b75cb68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "323e6184-5156-4bc3-b15d-a969c85d28c5",
   "metadata": {},
   "source": [
    "## 3.2 Benchmarks ## \n",
    "Now that we have both a determinized agent and an MCTS agent, let's do some benchmarking.\n",
    "\n",
    "We have provided you with a simple agent, `RolloutLookaheadAgent`: At each step, it performs several rollouts for each action and chooses the best one. Note that `RolloutLookaheadAgent` with `receding_horizon=0` becomes a naive agent that chooses action uniformly at random.\n",
    "\n",
    "Let's compare these agents: `RolloutLookaheadAgent(receding_horizon=0)`, `RolloutLookaheadAgent(receding_horizon=40)`, `FireMDPDeterminizedAStarAgent`, `MCTSAgent(iteration_budget=10, receding_horizon=40)` and `MCTSAgent(iteration_budget=50, receding_horizon=40)`. Please set unspecified parameters to their default values.\n",
    "\n",
    "Then, run the benchmarks. In particular:\n",
    "\n",
    "- For each environment in `get_problem`, run each agent at least 10 times in that environment.\n",
    "- Record the obtained total rewards for each run.\n",
    "- Record the average, standard deviation, min, and max rewards each agent obtains.\n",
    "Note that running the MCTS agent can take a while. In our experience, the benchmarking may take a few hours. Therefore, we recommend running the MCTS agent on a local laptop or desktop. If you choose to do so, you may change `MCTSAgent(iteration_budget=50)` to one with more iterations, such as `MCTSAgent(iteration_budget=100)` or `MCTSAgent(iteration_budget=500)` --- doing so should produce much better MCTS agent. You may also want to repeat each setting more than 10 times, such as 30 times --- doing so will reduce the effect of stochasticity in the experiments.\n",
    "\n",
    "Please prepare a table comparing the above agents' performances in the environments. We do not impose a format for the table, but you should prepare the table such that it is reasonably readable. Remember to indicate the experiment settings for the table (the parameters you used and the number of repetitions, etc.). Once you have the table, try to identify any interesting patterns from the table, and summarize your findings in words.\n",
    "\n",
    "Hints:\n",
    "- You may want to take a look at the `benchmark_agent` and `compare_agents` functions, which contains some boilerplate code to get you started.\n",
    "- In particular, you may also use the parameter `max_steps` of the above functions to limit the number of steps for each evaluation episode, if evaluation is taking too much time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ae9ba-6814-4ea6-8220-a85430966807",
   "metadata": {},
   "source": [
    "**Please submit a PDF of the results, including the table and a summarization of it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e98c0a",
   "metadata": {},
   "source": [
    "## Final Submission\n",
    "Your final submission to gradescope should include the following files:\n",
    "- `project01.ipynb`: Your completed notebook **with output from running each cell**. Make sure to save. If you made changes to any of the `.py` files, please include that as well.\n",
    "- `just_wait_determinized.mp4`: The animation of the successful run of the determinized agent in the `just_wait` MDP.\n",
    "- `the_choice_determinized.mp4`: The animation of the successful run of the determinized agent in the `the_choice` MDP.\n",
    "- `the_choice_mcts.mp4`: The animation of the successful run of the MCTS agent in the `the_choice` MDP.\n",
    "- A PDF of the results of the benchmarking, as described in the last part of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50421f0",
   "metadata": {},
   "source": [
    "## Feedback\n",
    "\n",
    "If you have any feedback for us, please complete [this form](https://forms.gle/58Juq1TDtxXKp11q7)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbf5bf9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "mp03.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Set 10: Machine Learning\n",
    "\n",
    "In this problem set, you will implement Naive Bayes, Logistic Regression, and Neural Network training.\n",
    "\n",
    "0. [Credit for Contributors (required)](#contributors)\n",
    "1. [Naive Bayes (45 points)](#problem1)\n",
    "    1. [Count Word Frequency (5 points)](#get_counts)\n",
    "    2. [Learn Parameters (20 points)](#NB_learn_params)\n",
    "    3. [Classify Message (10 points)](#NB_classify)\n",
    "    4. [Observations: Priors and Smoothing (10 points)](#NB_observations)\n",
    "2. [Logistic Regression (40 points)](#problem2)\n",
    "    1. [Loss Function (5 points)](#logistic_loss)\n",
    "    2. [Gradient Descent (30 points)](#gradient_descent)\n",
    "    3. [Learning Rate (5 points)](#learning_rate)\n",
    "3. [Neural Networks (30 points)](#problem3)\n",
    "    1. [Define Neural Network (15 points)](#define_NN)\n",
    "    2. [Prevent Overfitting (15 points)](#overfitting)\n",
    "4. [Homework survey (5 points)](#part4)\n",
    "    \n",
    "**120 points** total for Problem Set 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"contributors\"></a> Credit for Contributors\n",
    "\n",
    "List the various students, lecture notes, or online resouces that helped you complete this problem set:\n",
    "\n",
    "Ex: I worked with Bob on the cat activity planning problem.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Write your answer in the cell below this one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *(double click on this cell to delete this text and type your answer here)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to run the cell below to import the code needed for this assignment.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from utils import *\n",
    "\n",
    "# imports for autograder\n",
    "from principles_of_autonomy.grader import Grader\n",
    "from principles_of_autonomy.notebook_tests.pset_10 import TestPSet10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"problem1\"></a> Problem 1: Naive Bayes (45 points)\n",
    "\n",
    "In this problem, you will build a Naive Bayes classifier to detect spam. Follow the steps below to implement the required functions and test your classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are giving you a dataset of [5,572 SMS messages](https://archive.ics.uci.edu/dataset/228/sms+spam+collection), which we split into a training and a test set for you. Run the following and have a look at some datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean dataset. The clean step removes punctuation and makes everything lowercase.\n",
    "labels, messages = load_data('SMS_data')\n",
    "labels, messages = clean_data(labels, messages)\n",
    "\n",
    "# Look at the data.\n",
    "for i in range(5):\n",
    "    print(f\"{labels[i]} : {messages[i]}\")\n",
    "\n",
    "# Split into training and test sets.\n",
    "labels, messages = shuffle_data(labels, messages)\n",
    "(training_labels, training_messages), (test_labels, test_messages) = split_data(labels, messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, you should assume that each message $i$ has an unobserved class variable $C^i$ that takes the value \"spam\" or \"ham\". Let $\\{w_1 ... w_J\\}$ be the set of all words used in all messages. The features of each message are binary word occurrences: $Y^i_j$ is a binary random variable that takes on value 1 if $w_j$ occurs in message $i$, and 0 if it does not. You can imagine that words like \"free\", \"win\", and so on would be very likely to occur in spam, but not as likely in ham.\n",
    "\n",
    "The goal is to classify messages using the Naive Bayes framework. The joint probability of class labels and features is given by:\n",
    "\n",
    "$$\n",
    "P(C^1=c^1, \\ldots, C^N=c^N, Y^1_1=y^1_1, \\ldots, Y^N_J=y^N_J) =\n",
    "\\prod_{i=1}^N \\left[ P(C^i=c^i) \\prod_{j=1}^J P(Y^i_j=y^i_j \\mid c^i) \\right]\n",
    "$$\n",
    "\n",
    "We will assume that:\n",
    "\n",
    "$$\n",
    "P(Y^i_j=y^i_j \\mid c^i) =\n",
    "\\begin{cases} \n",
    "    \\text{Bernoulli}(q_j), & \\text{if } c^i = \\text{spam} \\\\\n",
    "    \\text{Bernoulli}(p_j), & \\text{if } c^i = \\text{ham}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "P(C^i=c^i) =\n",
    "\\begin{cases}\n",
    "    s, & \\text{if } c^i = \\text{spam} \\\\\n",
    "    1 - s, & \\text{if } c^i = \\text{ham}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "To avoid working with multiplying small probabilities together, weâ€™ll work with log probabilities instead. The equation then becomes:\n",
    "$$\n",
    "\\log P(C^1=c^1, \\ldots, C^N=c^N, Y^1_1=y^1_1, \\ldots, Y^N_J=y^N_J) =\n",
    "\\sum_{i=1}^N \\left[ \\log P(C^i=c^i) + \\sum_{j=1}^J \\log P(Y^i_j=y^i_j \\mid c^i) \\right]\n",
    "$$\n",
    "\n",
    "The maximum likelihood estimate for the parameter $p_j$ is just the fraction of ham messages that contain the word $w_j$, and similarly, the maximum likelihood estimate for $q_j$ is the fraction of spam messages that contain the word $w_j$. The maximum likelihood estimate for $s$ is the fraction of training documents that are spam. **Our goal is to estimate these parameters and build a classification function** that can classify new messages as spam or ham with the parameters we estimate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"get_counts\"></a> Count Word Frequency (5 points)\n",
    "\n",
    "The first step to estimating the Naive Bayes parameters is counting the number of messages each word occurs in. Implement a function `get_counts` that takes in the training messages and returns a dictionary whose keys are words, and whose values are the number of messages the key occurred in. Words are all collections of characters separated by whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Implement the function `get_counts(messages)` below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(messages):\n",
    "    \"\"\"\n",
    "    Computes counts for each word that occurs in the messages.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    messages : a list of sms messages.\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    A dict whose keys are words, and whose values are the number of files the key occurred in.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grader.run_single_test_inline(TestPSet10, \"test_01\", locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at your counts ordered by value. You should see common English words (e.g. prepositions) at the top of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = get_counts(training_messages)\n",
    "sorted_word_counts = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "sorted_word_counts[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"NB_learn_params\"></a> Parameter Learning (20 points)\n",
    "\n",
    "You will now implement 2 functions to estimate the Naive Bayes parameters. \n",
    "\n",
    "First, implement `get_log_probabilities`, which, given a list of messages, computes the log of the fraction of messages each word occurrs in. To avoid issues with unseen words, the function also takes a smoothing parameter `k`. You should use [**Laplace smoothing**](https://en.wikipedia.org/wiki/Additive_smoothing) to compute the smoothed frequencies of each word. The function should return a dictionary whose keys are words, and whose values are the log of the smoothed estimate of the fraction of messages the key occurred in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Implement the functions `get_log_probabilities(messages, k)` and `learn_distributions(messages_by_category, k)` below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probabilities(messages, k):\n",
    "    \"\"\"\n",
    "    Computes log-frequencies for each word that occurs in the messages.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    messages : a list of sms messages.\n",
    "    k : Laplace smoothing parameter.\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    log_prob : a **defaultdict** dictionary whose keys are words, and whose values are the log of the smoothed estimate of the fraction of messages the key occurred in.\n",
    "\n",
    "    Hint\n",
    "    ------\n",
    "    1. The `get_counts` helper function you wrote earlier should be helpful here.\n",
    "    2. To handle missing words, you should use defaultdict from collections (imported at the top of the file) as your dictionary.\n",
    "       This allows you to set a default value for keys not in the dictionary, which will come in handy later.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grader.run_single_test_inline(TestPSet10, \"test_02\", locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement `learn_distributions`, which learns the parameters for each conditional distribution $P(Y_j|C)$ and prior distribution $P(C)$ in the model. The function takes a list `messages_by_category` which is a two-element list: the first element is a list of spam messages and the second element is a list of ham messages. The function also takes `k` which you should use to call `get_log_probabilities` for obtaining smoothed frequencies. You should return a tuple containing: 1. a list with a smoothed estimate for $\\log P(Y_j=w_j\\mid C=spam)$ (as a dictionary) and a smoothed estimate for $\\log P(Y_j=w_j\\mid C=ham)$ (also as a dictionary); 2. a list with estimates for the log-probabilities for each class, i.e. $\\log P(C=spam)$ and $\\log P(C=ham)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_distributions(messages_by_category, k):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    -----\n",
    "    messages_by_category :  A two-element list. The first element is a list of spam messages, \n",
    "                            and the second element is a list of ham (non-spam) messages.\n",
    "    k : Laplace smoothing parameter.\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    (log_probs_by_category, log_prior_by_category)\n",
    "\n",
    "    log_probs_by_category : A list whose first element is a smoothed estimate for log P(y=w_j|c=spam) (as a dict,\n",
    "                            just as in get_log_probabilities above), and whose second element is the same for c=ham.\n",
    "\n",
    "    log_prior_by_category : A list of estimates for the log-probabilities for each class: [est. for log P(c=spam), est. for log P(c=ham)]\n",
    "    \"\"\"\n",
    "    log_probs_by_category = []\n",
    "    log_prior_by_category = []\n",
    "    raise NotImplementedError()\n",
    "    return (log_probs_by_category, log_prior_by_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grader.run_single_test_inline(TestPSet10, \"test_03\", locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"NB_classify\"></a> Classification (10 points)\n",
    "\n",
    "Implement the function `classify_message`, which classifes a new message using the Maximum A-Posteriori, or MAP, rule. The MAP rule is almost the same as the MLE rule except it additionally multiplies the data likelihood by the prior. The function takes in a string message and the probabilities estimated in the previous part, then uses the equation in the problem description to compute the log probabilities for each category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Implement the function `classify_message(message, log_probs_by_category, log_prior_by_category)` below.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Hint: for classifying a single message, $N$ and $J$ have different values from those in the training equation above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_message(message, log_probs_by_category, log_prior_by_category):\n",
    "    \"\"\"\n",
    "    Classifies a new message as spam or ham using the MAP rule.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    message : a string representing the new message to classify.\n",
    "    log_probs_by_category : a list whose first element is a dict of log P(w_j|c=spam), \n",
    "                            and second element is a dict of log P(w_j|c=ham).\n",
    "    log_prior_by_category : a list of log-priors [log P(c=spam), log P(c=ham)].\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    A string: \"spam\" or \"ham\", indicating the classification result.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test your classification code here.\"\"\"\n",
    "Grader.run_single_test_inline(TestPSet10, \"test_04\", locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate your classification code on the test set. You should expect about 80% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract spam and ham messages from the training data.\n",
    "spam_messages = [msg for lbl, msg in zip(training_labels, training_messages) if lbl == 'spam']\n",
    "ham_messages = [msg for lbl, msg in zip(training_labels, training_messages) if lbl == 'ham']\n",
    "\n",
    "# Set Laplace smoothing parameter.\n",
    "k = 1\n",
    "\n",
    "# Training: learn distribution parameters from the training data.\n",
    "(log_probs_by_category, log_prior_by_category) = learn_distributions([spam_messages, ham_messages], k)\n",
    "\n",
    "# Here, columns and rows are indexed by 0 = 'spam' and 1 = 'ham'.\n",
    "# Rows correspond to true label, columns correspond to guessed label.\n",
    "performance_measures = np.zeros([2,2])\n",
    "\n",
    "### Classify and measure performance on the test set.\n",
    "for label, message in zip(test_labels, test_messages):\n",
    "    predicted_label = classify_message(message, log_probs_by_category, log_prior_by_category)\n",
    "    true_index = 0 if label == \"spam\" else 1\n",
    "    guessed_index = 0 if predicted_label == \"spam\" else 1\n",
    "    performance_measures[true_index, guessed_index] += 1\n",
    "correct = np.diag(performance_measures)\n",
    "totals = np.sum(performance_measures, axis=1)\n",
    "accuracy = (correct[0] + correct[1]) / np.sum(performance_measures)\n",
    "print(f\"You correctly classified {correct[0]} out of {totals[0]} spam messages, and {correct[1]} out of {totals[1]} ham messages. Accuracy: {accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"NB_observations\"></a> Observations (10 points)\n",
    "\n",
    "Estimating $s$ from data is heavily reliant on the sizes of the data. In the real world, it's often difficult to find good training examples for ham, since nobody wants to give out their private email for the world to read. As a result, spam datasets often have many more spam examples than ham examples. By fixing $s$ manually, we can adjust how much the algorithm favors catching spam at the expense of falsely flagging a ham message. Try setting $s$ to a few values manually, and briefly explain what happens to your performance as $s$ increases and decreases.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Discuss your results in the cell below**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *(double click on this cell to delete this text and type your answer here)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the Laplace smoothing parameter is critical for handling unseen words during testing by assigning them a small nonzero probability. However, it also imposes a uniform prior, which smoothens the estimated probabilities of observed words. In the previous code block, we estimated the Naive Bayes parameters using $k = 1$. Experiment with a range of $k$ values and identify the value of $k$ that results in the highest accuracy. Briefly explain how $k$ affects the model's performance.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Discuss your results in the cell below**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *(double click on this cell to delete this text and type your answer here)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"problem2\"></a> Problem 2: Logistic Regression (40 points)\n",
    "\n",
    "In this problem, you will implement **Logistic Regression** to classify points in a 2D space into two classes. We have generated a synthetic dataset of 500 samples, each with two features; the data can be summarized as a $500\\times2$ matrix `X` representing the input features and a 1D array `y` representing the true labels corresponding to each datapoint. We split the data for you into a training set and test set. Run the following cell to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data.\n",
    "X, y = make_classification(\n",
    "    n_samples=500, n_features=2, n_informative=2, n_redundant=0, \n",
    "    n_clusters_per_class=1, class_sep=1.5, flip_y=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
    "plt.title(\"Synthetic Dataset for Logistic Regression\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n",
    "\n",
    "# Augment the feature matrix X by adding a bias column of ones.\n",
    "X = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"logistic_loss\"></a> Loss Function (5 points)\n",
    "\n",
    "The goal of logistic regression is to find the optimal weights that minimize the difference between predicted probabilities and true labels. To achieve this, we use gradient descent, an optimization algorithm that iteratively adjusts the model's parameters. Before we dive into implementing gradient descent, you will first implement three functions that we'll make use of in our model parameter fitting. \n",
    "\n",
    "First, implement the `sigmoid` function, which takes a real value as input and returns a value between 0 and 1, representing the probability of belonging to a class. This should be one line of code implementing the following: \n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Next, implement `binary_cross_entropy`, which is the loss function the gradient descent algorithm will optimize. We talked about Binary Cross Entropy in the neural networks lecture, but the same principle applies for logistic regression: given an array of prediction probabilities `y_pred` and the corresponding true class labels `y_true`, BCE measures how far off the model predictions are. This should also be one line of code implementing:\n",
    "\n",
    "$$\n",
    "\\text{Loss}(y, \\hat{y}) = - \\frac{1}{N} \\sum_{i=1}^N \\left( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right)\n",
    "$$\n",
    "\n",
    "Finally, implement `loss_gradient`, which calculates the gradients of the loss function with respect to the model's parameters. The gradient for logistic regression is $\\nabla_w \\text{Loss} = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i) \\mathbf{x}_i$, where $\\mathbf{x}_i$ is the feature vector for the i-th sample. To see how we got this formula, you can try taking the gradient of the Loss equation above w.r.t. the weights by hand, as an optional (but very educational! :) ) exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Implement the functions `sigmoid(z)`, `binary_cross_entropy(y_true, y_pred)`, and loss_gradient(X, y, y_pred) below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z.\n",
    "\n",
    "    Input:\n",
    "    - z : A scalar or numpy array of any size.\n",
    "\n",
    "    Output:\n",
    "    - The sigmoid of z.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Binary cross-entropy loss\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the binary cross-entropy loss.\n",
    "\n",
    "    Inputs:\n",
    "    - y_true : A numpy array of true binary labels (0 or 1).\n",
    "    - y_pred : A numpy array of predicted probabilities (0 <= y_pred <= 1).\n",
    "\n",
    "    Output:\n",
    "    - The binary cross-entropy loss.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15  # To avoid log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def loss_gradient(X, y, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the gradients of the loss with respect to the augmented weights.\n",
    "\n",
    "    Inputs:\n",
    "    - X : A numpy array of shape (N, D+1), the augmented feature matrix.\n",
    "    - y : A numpy array of shape (N,), the binary labels (0 or 1).\n",
    "    - y_pred : A numpy array of shape (N,), the predicted probabilities.\n",
    "\n",
    "    Output:\n",
    "    - dw : A numpy array of shape (D+1,), the gradient of the loss with respect to w.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test your loss function code here.\"\"\"\n",
    "Grader.run_single_test_inline(TestPSet10, \"test_05\", locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"gradient_descent\"></a> Gradient Descent (30 points)\n",
    "\n",
    "You will now implement the **Gradient Descent** update rule for logistic regression. In class, we discussed that we can estimate the weights using Maximum Likelihood Estimation, and we wrote logistic regression as an optimization problem where we maximize the log-likelihood of the data given the parameters with gradient ascent. In practice, it's more common to convert the problem into a minimization problem (by negating the log-likelihood; sometimes this is called the *nll* in shorthand notation) and run gradient *descent*. In this problem, you will:\n",
    "1. Implement batch gradient descent (process all data at once in a single update per epoch).\n",
    "2. Implement stochastic gradient descent (process one data point at a time from the shuffled dataset).\n",
    "3. Implement mini-batch gradient descent (process small batches of data from the shuffled dataset).\n",
    "\n",
    "We provide you with the outer training loop, visualization functions, and evaluation inside `train_and_plot`. You should implement the functions `batch_gradient_descent`, `stochastic_gradient_descent`, and `mini_batch_gradient_descent`, each of which execute a single *epoch*, or iteration, or gradient descent. As a reminder, batch gradient descent makes a single update per epoch, stochastic gradient descent makes $N$ updates per epoch, and mini-batch gradient descent makes $N/J$ updates per epoch (where $J$ is the batch size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Implement the functions `batch_gradient_descent(X, y, w, alpha)`, `stochastic_gradient_descent(X, y, w, alpha)`, `mini_batch_gradient_descent(X, y, w, alpha, batch_size)` below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_plot(X_train, y_train, X_test, y_test, gd_type, alpha, epochs=100, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train the logistic regression model using the specified gradient descent method and plot the loss curve.\n",
    "\n",
    "    Inputs:\n",
    "    - X_train : Training data (N, D+1).\n",
    "    - y_train : Training labels (N,).\n",
    "    - X_test : Test data (M, D+1).\n",
    "    - y_test : Test labels (M,).\n",
    "    - gd_type : A string, either 'batch', 'stochastic', or 'mini_batch'.\n",
    "    - alpha : A float, the learning rate.\n",
    "    - epochs : An integer, the number of training epochs.\n",
    "    - batch_size : An integer, the size of each mini-batch (used only for 'mini_batch').\n",
    "\n",
    "    Outputs:\n",
    "    - None. Prints accuracy and plots the loss curve.\n",
    "    \"\"\"\n",
    "    # Initialize parameters.\n",
    "    w = np.zeros(X_train.shape[1])\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if gd_type == \"batch\":\n",
    "            w = batch_gradient_descent(X_train, y_train, w, alpha)\n",
    "        elif gd_type == \"stochastic\":\n",
    "            w = stochastic_gradient_descent(X_train, y_train, w, alpha)\n",
    "        elif gd_type == \"mini_batch\":\n",
    "            w = mini_batch_gradient_descent(X_train, y_train, w, alpha, batch_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid gd_type. Choose 'batch', 'stochastic', or 'mini_batch'.\")\n",
    "\n",
    "        # Compute loss for the current epoch for plotting purposes.\n",
    "        y_pred_train = sigmoid(np.dot(X_train, w))\n",
    "        loss = binary_cross_entropy(y_train, y_pred_train)\n",
    "        losses.append(loss)\n",
    "\n",
    "    # Plot the loss curve.\n",
    "    plt.plot(losses, label=f\"{gd_type} GD\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Loss Curve for {gd_type.capitalize()} GD\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot decision boundary.\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for label, color in zip([0, 1], ['blue', 'red']):\n",
    "        plt.scatter(X_train[y_train == label, 0], X_train[y_train == label, 1], \n",
    "                    label=f\"Class {label}\", alpha=0.7, edgecolors='k', c=color)\n",
    "    x_vals = np.linspace(X_train[:, 0].min(), X_train[:, 0].max(), 100)\n",
    "    y_vals = -(w[0] / w[1]) * x_vals - (w[2] / w[1])  # Solve for x2\n",
    "    plt.plot(x_vals, y_vals, label=\"Decision Boundary\", color=\"black\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.title(\"Decision Boundary and Data\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred_test = sigmoid(np.dot(X_test, w)) >= 0.5\n",
    "    accuracy = np.mean(y_pred_test == y_test)\n",
    "    print(f\"{gd_type.capitalize()} GD Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Batch Gradient Descent\n",
    "def batch_gradient_descent(X, y, w, alpha):\n",
    "    \"\"\"\n",
    "    Perform one step of batch gradient descent using loss_gradient.\n",
    "\n",
    "    Inputs:\n",
    "    - X : A numpy array of shape (N, D+1), the augmented feature matrix.\n",
    "    - y : A numpy array of shape (N,), the binary labels (0 or 1).\n",
    "    - w : A numpy array of shape (D+1,), the augmented weights.\n",
    "    - alpha : A float, the learning rate.\n",
    "\n",
    "    Output:\n",
    "    - Updated weights w.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test your BATCH gradient descent code here. NOTE: stochastic and mini_batch are not tested\"\"\"\n",
    "Grader.run_single_test_inline(TestPSet10, \"test_06\", locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, w, alpha):\n",
    "    \"\"\"\n",
    "    Perform one epoch of stochastic gradient descent using loss_gradient.\n",
    "    Goes through the entire dataset randomly.\n",
    "\n",
    "    Inputs:\n",
    "    - X : A numpy array of shape (N, D+1), the augmented feature matrix.\n",
    "    - y : A numpy array of shape (N,), the binary labels (0 or 1).\n",
    "    - w : A numpy array of shape (D+1,), the augmented weights.\n",
    "    - alpha : A float, the learning rate.\n",
    "\n",
    "    Output:\n",
    "    - Updated weights w.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Mini-Batch Gradient Descent\n",
    "def mini_batch_gradient_descent(X, y, w, alpha, batch_size):\n",
    "    \"\"\"\n",
    "    Perform one epoch of mini-batch gradient descent using loss_gradient.\n",
    "    Goes through the entire dataset in random mini-batches.\n",
    "\n",
    "    Inputs:\n",
    "    - X : A numpy array of shape (N, D+1), the augmented feature matrix.\n",
    "    - y : A numpy array of shape (N,), the binary labels (0 or 1).\n",
    "    - w : A numpy array of shape (D+1,), the augmented weights.\n",
    "    - alpha : A float, the learning rate.\n",
    "    - batch_size : An integer, the number of samples in each mini-batch.\n",
    "\n",
    "    Output:\n",
    "    - Updated weights w.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's test your code and visualize the results (20 points):\n",
    "We will be grading the below plots. **Do not modify the code below**. You should expect accuracies greater than 80\\% for all three methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT MODIFY\n",
    "epochs = 100\n",
    "alpha = 0.01\n",
    "batch_size = 32\n",
    "\n",
    "train_and_plot(X_train, y_train, X_test, y_test, gd_type=\"batch\", alpha=alpha, epochs=epochs)\n",
    "train_and_plot(X_train, y_train, X_test, y_test, gd_type=\"stochastic\", alpha=alpha, epochs=epochs)\n",
    "train_and_plot(X_train, y_train, X_test, y_test, gd_type=\"mini_batch\", alpha=alpha, epochs=epochs, batch_size=batch_size)\n",
    "### DO NOT MODIFY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"learning_rate\"></a> Learning Rate (5 points)\n",
    "Try varying the learning rate parameter `alpha` in the cell **below**, e.g. 0.001, 0.005, 0.01, 0.05, 0.1, 0.5.\n",
    "- What are the best learning rates for each of the 3 gradient descent variants? What happens when the learning rate is too small? What about too large?\n",
    "- Which method is more robust to large learning rates?\n",
    "- Which method converges fastest when the learning rate is optimal?\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Discuss your results below**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *(double click on this cell to delete this text and type your answer here)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODIFY THESE!\n",
    "epochs = 100\n",
    "alpha = 0.01\n",
    "batch_size = 32\n",
    "\n",
    "train_and_plot(X_train, y_train, X_test, y_test, gd_type=\"batch\", alpha=alpha, epochs=epochs)\n",
    "train_and_plot(X_train, y_train, X_test, y_test, gd_type=\"stochastic\", alpha=alpha, epochs=epochs)\n",
    "train_and_plot(X_train, y_train, X_test, y_test, gd_type=\"mini_batch\", alpha=alpha, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"problem3\"></a> Problem 3: Neural Networks (30 points)\n",
    "\n",
    "In this problem, you will define a **Neural Network**, train it with a given dataset, and explore overfitting prevention techniques, all with PyTorch. The goal of this problem is to take you through the journey of what training a neural network with modern autodiff packages looks like. Since PyTorch is almost a new language, you won't have to write too much code and when you do, we will provide guidance. You may want to befriend the [PyTorch documentation](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate a dataset and split it into a training set, validation set, and test set. The difference between validation and test is that test is used for the final evaluation of the model (like before), and the validation set is meant for picking hyperparameters or early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset.\n",
    "X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert this dataset to PyTorch tensors, which are specialized data structures that are very similar to numpy arrays, but with additional features that make them ideal for deep learning tasks (can run on GPUs or other hardware accelerators, but not for this homework)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors.\n",
    "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n",
    "X_val, y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long)\n",
    "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create DataLoaders, which will help handle the dataset batching that you had to do manually in mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader for batching.\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"define_NN\"></a> Define Neural Network (15 points)\n",
    "\n",
    "Your task is to define a **3-layer fully connected neural network** for binary classification using PyTorch. The structure of the network is:\n",
    "1. Input â†’ Hidden Layer 1 â†’ ReLU Activation.\n",
    "2. Hidden Layer 1 output (after ReLU) â†’ Hidden Layer 2 â†’ ReLU Activation.\n",
    "3. Hidden Layer 2 output (after ReLU)â†’ Output Layer.\n",
    "\n",
    "Complete the class `SimpleNN` below using `nn.Linear` for the fully connected layers and `nn.ReLU` for activations. Some helpful documentation:\n",
    "\n",
    "1. `nn.Linear` usage: `nn.Linear(in_features, out_features)` implements a fully connected layer where `in_features` is the size of the input vector and `out_features` is the size of the output vector.\n",
    "2. `nn.ReLU` usage: `nn.ReLU()` is an activation that introduces non-linearity in the model by applying the ReLU function: $\\text{ReLU}(x) = \\max(0, x)$.\n",
    "3. `__init__` vs `forward`: `__init__` defines the network layers as class attributes, whereas `forward` specifies how data flows through the layers (i.e. how to combine/nest the layers together to go from input `x` to output)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Implement the `__init__` and `forward` methods below. (5 points)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print your model. Make sure the in/out dimensions are compatible from layer to layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(\n",
    "    input_size=2, \n",
    "    hidden_size1=16, \n",
    "    hidden_size2=8, \n",
    "    output_size=2\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test your neural network code here.\"\"\"\n",
    "Grader.run_single_test_inline(TestPSet10, \"test_07\", locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Answer the following multiple choice questions (one answer per question, 10 points):\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why might increasing the number of hidden layers improve performance?**\n",
    "\n",
    "- (a) It helps the model capture more complex, non-linear patterns.\n",
    "- (b) It reduces the risk of overfitting.\n",
    "- (c) It makes training faster.\n",
    "- (d) It is always better than adding more neurons to a single layer.\n",
    "\n",
    "---\n",
    "\n",
    "**Why donâ€™t we apply an activation function (e.g., ReLU, sigmoid) to the output layer when using `CrossEntropyLoss`?**\n",
    "\n",
    "- (a) The output layer doesnâ€™t need to be non-linear.\n",
    "- (b) The `CrossEntropyLoss` function internally applies `softmax`.\n",
    "- (c) It makes training faster.\n",
    "- (d) Activations donâ€™t work well with binary classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer for the two above multiple multiple choice questions in the following tuple \n",
    "# # as (q1_ans, q2_ans) e.g. ('a', 'a')\n",
    "mc_answer_set_1 = (None, None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test your multiple choice answers here.\"\"\"\n",
    "Grader.run_single_test_inline(TestPSet10, \"test_08\", locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"overfitting\"></a> Training with Early Stopping and Weight Regularization (15 points)\n",
    "\n",
    "In this part, you'll train the neural network you just defined with the dataset we gave you. We provide all of the training code (again, the important thing is understanding the training logic), but you will have to implement the `early_stopping` function and explore weight regularization in the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the (many) beauties of PyTorch is that we don't need to manually define the loss function or optimization algorithm like we did in the logistic regression problem. Instead, we only need these 2 lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Loss for classification tasks\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we provide the training loop. Implement the `early_stopping` function, which given the current validation loss `val_loss` and the best validation loss so far `best_val_loss`, it determines if the loss hasn't been decreasing for longer than `patience` epochs, and if so, stops training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Implement the `early_stopping` function below (5 points)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=100, patience=10):\n",
    "    \"\"\"\n",
    "    Train a PyTorch model using SGD and apply early stopping.\n",
    "\n",
    "    Inputs:\n",
    "    - model: The PyTorch model to train.\n",
    "    - train_loader: DataLoader for the training set.\n",
    "    - val_loader: DataLoader for the validation set.\n",
    "    - criterion: Loss function (e.g., nn.CrossEntropyLoss).\n",
    "    - optimizer: Optimizer (e.g., torch.optim.SGD).\n",
    "    - epochs: Number of training epochs.\n",
    "    - patience: Number of epochs to wait for improvement in validation loss before stopping.\n",
    "\n",
    "    Outputs:\n",
    "    - train_losses: List of average training losses per epoch.\n",
    "    - val_losses: List of average validation losses per epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Training parameters\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float(\"inf\")  # For early stopping\n",
    "    counter = 0  # Early stopping counter\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train() # Sets the model in training phase (lets gradients flow).\n",
    "        epoch_train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()           # Reset gradients\n",
    "            outputs = model(X_batch)        # Forward pass\n",
    "            loss = criterion(outputs, y_batch)  # Compute loss\n",
    "            loss.backward()                 # Backward pass (compute gradients)\n",
    "            optimizer.step()                # Update weights\n",
    "            epoch_train_loss += loss.item() # Accumulate training loss\n",
    "        train_losses.append(epoch_train_loss / len(train_loader))\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval() # Sets the model in evaluation phase (freezes the network, so gradients don't flow).\n",
    "        epoch_val_loss = 0\n",
    "        with torch.no_grad():  # No gradient computation during evaluation\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                epoch_val_loss += loss.item()\n",
    "        val_losses.append(epoch_val_loss / len(val_loader))\n",
    "\n",
    "        # Print progress\n",
    "        if (epoch+1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "        # Apply early stopping\n",
    "        best_val_loss, counter, stop_training = early_stopping(val_losses[-1], best_val_loss, counter, patience)\n",
    "        if val_losses[-1] == best_val_loss:  # Save the model only if val_loss improves\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        if stop_training:\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses\n",
    "        \n",
    "def early_stopping(val_loss, best_val_loss, counter, patience):\n",
    "    \"\"\"\n",
    "    Implements early stopping logic.\n",
    "\n",
    "    Inputs:\n",
    "    - val_loss: Current validation loss.\n",
    "    - best_val_loss: Best validation loss seen so far.\n",
    "    - counter: Current count of consecutive epochs without improvement.\n",
    "    - patience: Number of epochs to wait before stopping.\n",
    "\n",
    "    Outputs:\n",
    "    - best_val_loss: Updated best validation loss.\n",
    "    - counter: Updated counter value.\n",
    "    - stop_training: Boolean flag indicating whether to stop training.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test your early stopping code here.\"\"\"\n",
    "Grader.run_single_test_inline(TestPSet10, \"test_09\", locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the training loop, we can evaluate the model and visualize the loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these parameters.\n",
    "decay = 0.001\n",
    "patience = 10\n",
    "\n",
    "# Initialize the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=decay)  # Adjust weight_decay as needed\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, epochs=100, patience=patience\n",
    ")\n",
    "\n",
    "# Plot loss curves\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Load the best model for evaluation\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on the test set\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get class predictions\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Try varying the early stopping `patience` parameter and the weight regularization parameter `decay`. Then mark which of the following statements are true in the code below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple choice (10 points)\n",
    "\n",
    "Select which of the following statements are true (any number of them can be true):\n",
    "\n",
    "**1. How does the patience parameter in early stopping affect training?**\n",
    "- (a) Longer patience reduces the chance of stopping too early.\n",
    "- (b) Shorter patience stops training faster but risks underfitting.\n",
    "- (c) Patience only affects validation loss, not training.\n",
    "\n",
    "---\n",
    "\n",
    "**2. If validation loss fluctuates, how might patience help stabilize early stopping?**\n",
    "- (a) Patience allows the model to continue training despite small fluctuations in validation loss.\n",
    "- (b) Patience has no effect if validation loss is fluctuating.\n",
    "- (c) Fluctuations in validation loss always indicate overfitting.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**3. Why does early stopping use validation loss instead of training loss?**\n",
    "- (a) Validation loss better reflects generalization to unseen data.\n",
    "- (b) Training loss decreases continuously and doesnâ€™t indicate overfitting.\n",
    "- (c) Validation loss is typically noisier, making it better for monitoring.\n",
    "\n",
    "---\n",
    "\n",
    "**4. What effect does weight decay have on training?**\n",
    "- (a) It penalizes large weights, reducing overfitting.\n",
    "- (b) It makes the training loss larger.\n",
    "- (c) It speeds up convergence.\n",
    "- (d) It has no effect on validation loss.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**5. What might happen if weight decay is set too high or too low?**\n",
    "- (a) If too high, the model might underfit the data.\n",
    "- (b) If too low, the model might overfit the training data.\n",
    "- (c) Weight decay has no significant impact on training or validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark the statements that are true for each of the above questions. \n",
    "# E.g. if A in question 1 is true set a=True in the q1 dict.\n",
    "\n",
    "mc_answer_set_2 = dict(\n",
    "    q1 = dict(a=False, b=False, c=False),\n",
    "    q2 = dict(a=False, b=False, c=False),\n",
    "    q3 = dict(a=False, b=False, c=False),\n",
    "    q4 = dict(a=False, b=False, c=False, d=False),\n",
    "    q5 = dict(a=False, b=False, c=False),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test your multiple choice answers here.\"\"\"\n",
    "Grader.run_single_test_inline(TestPSet10, \"test_10\", locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"part4\"></a> Time Spent on Pset (5 points)\n",
    "\n",
    "Please use [this form](https://forms.gle/GJ4MkpMFkxMAPL2D8) to tell us how long you spent on this pset. After you submit the form, the form will give you a confirmation word. Please enter that confirmation word below to get an extra 5 points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_confirmation_word = \"\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grader.grade_output([TestPSet10], [locals()], \"results.json\")\n",
    "Grader.print_test_results(\"results.json\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

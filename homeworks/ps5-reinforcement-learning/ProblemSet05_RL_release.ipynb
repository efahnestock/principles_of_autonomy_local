{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Set 05: Reinforcement Learning\n",
    "\n",
    "In this problem set, you will implement model-free approaches for reinforcement learning.\n",
    "\n",
    "\n",
    "0. [Credit for Contributors (required)](#contributors)\n",
    "\n",
    "1. [Passive Reinforcement Learning (60 points)](#problem1)\n",
    "    1. [Direct Evaluation (20 points)](#direct_evaluation)\n",
    "    2. [Sample Sensitivity (10 points)](#sample_sensitivity)\n",
    "    3. [Temporal Difference Learning (20 points)](#temporal_difference)\n",
    "    4. [Learning Rate Sensitivity (10 points)](#learning_rate)\n",
    "2. [Active Reinforcement Learning (35 points)](#problem2)\n",
    "    1. [Q-Learning (20 points)](#Qlearning)\n",
    "    2. [Epsilon-Greedy Q-Learning (10 points)](#epsilon_greedy)\n",
    "    3. [Exploration vs. Exploitation (5 points)](#exploration)\n",
    "3. [Homework survey (5 points)](#part3)\n",
    "    \n",
    "**100 points** total for Problem Set 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"contributors\"></a> Credit for Contributors\n",
    "\n",
    "List the various students, lecture notes, or online resouces that helped you complete this problem set:\n",
    "\n",
    "Ex: I worked with Bob on the cat activity planning problem.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Write your answer in the cell below this one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *(double click on this cell to delete this text and type your answer here)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to run the cell below to import the code needed for this assignment.\n",
    "from __future__ import division\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from grid import MDPGrid, generate_mdp_plot, generate_grid_plot\n",
    "from mdp_utils import *\n",
    "\n",
    "# imports for autograder\n",
    "from principles_of_autonomy.grader import Grader\n",
    "from principles_of_autonomy.notebook_tests.pset_5 import TestPSet5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"problem1\"></a> Problem 1: Passive Reinforcement Learning (60 points)\n",
    "\n",
    "In this problem, you will implement the Passive Reinforcement Learning approaches we saw in class: Direct Evaluation and Temporal Difference Learning.\n",
    "\n",
    "The problems in this problem set are based on the simple MDP class from Problem Set 4 defined in the `mdp_utils.py` file. Feel free to open Problem Set 4 for a reminder of the MDP definition and the grid world our robot operates in. This week, however, we assume that we don't know the transition function T or the reward function R. Instead, we will implement model-free reinforcement learning methods to calculate the Values and/or Q-Values.\n",
    "\n",
    "We provide you with a `generate_episodes` function that generates episodes from an agent acting according to a policy in the MDP:\n",
    "\n",
    "```python\n",
    "def generate_episodes(mdp, policy, num_episodes=5, max_steps=15)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the simple MDP from last week and visualize it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "goal = (2, 2)\n",
    "obstacles = [(0, 1)]\n",
    "\n",
    "# Build MDP with p=0.8 and gamma=0.8. Use default rewards.\n",
    "mdp = build_mdp(n, 0.8, obstacles, goal, 0.8)\n",
    "\n",
    "# Visualize the MDP:\n",
    "# 1. Create grid for plotting.\n",
    "g = MDPGrid(n, n)\n",
    "axes = g.draw()\n",
    "# 2. Draw goal and obstacle cells.\n",
    "g.draw_cell_circle(axes, goal, color='g')\n",
    "for ob in obstacles:\n",
    "    g.draw_cell_circle(axes, ob, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a policy to follow when we generate episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple policy that always moves right.\n",
    "policy = {s: 'right' for s in mdp.S}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate some episodes according to this policy in the MDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = generate_episodes(mdp, policy, num_episodes=10, max_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following print block to look at the generated episodes. Notice how they have different lengths because they reach terminal states at different times. Notice also how the stochasticity of the MDP manifests itself in the samples of an episode: sometimes you take an action and end up where you intended; other times the action takes you in a different state. The ratio of these outcomes is implicit from the transition probabilities of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each episode is a list of (state, action, state', reward) tuples\n",
    "for i, episode in enumerate(episodes):\n",
    "    print(f\"Episode {i+1} of length {len(episode)}: \".format(i, episode), episode, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"direct_evaluation\"></a> Direct Evaluation (20 points)\n",
    "\n",
    "In this part of the problem, you will implement **Direct Evaluation** (also called Monte-Carlo Evaluation). Your function should have the following signature:\n",
    "```python\n",
    "def direct_evaluation(episodes, gamma)\n",
    "```\n",
    "\n",
    "The function takes in a list of episodes representing different experiences of the agent acting according to a fixed policy in the MDP, and the discount factor gamma. The function should return a Python dictionary with the estimated value for each state in the MDP.\n",
    "\n",
    "In general Direct Evaluation has 2 variants: First-Visit and Every-Visit. In First-Visit, we only add a sample of discounted rewards to the value estimate whenever we encounter a state for the first time in the episode. In Every-Visit, we add a sample *every* time we encounter a state (even if it's repeated). You should implement the First-Visit variant of Direct Evaluation that we talked about in class. For this, you'll find it useful to keep track of states visited so far in the episode with a `visited_states` list (that you reset whenever you move on to the next episode)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Implement the function `direct_evaluation(episodes, gamma)` below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_evaluation(episodes, gamma):\n",
    "    \"\"\"\n",
    "    Direct Evaluation method to estimate the value function for each state for a fixed policy.\n",
    "    \n",
    "    Args:\n",
    "        episodes (list): A list of episodes. Each episode is a list of (state, action, state', reward) tuples.\n",
    "        gamma (float): The discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        V: A dictionary mapping states to their estimated value V(s).\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out how your code performs on the initial grid world we defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code for direct evaluation with the example from before and a policy that always goes right.\n",
    "\n",
    "# Build MDP with p=0.8 and gamma=0.8. Use default rewards.\n",
    "n = 3\n",
    "goal = (2, 2)\n",
    "obstacles = [(0, 1)]\n",
    "mdp = build_mdp(n, 0.8, obstacles, goal, 0.8)\n",
    "\n",
    "# Create simple policy that always moves right.\n",
    "policy = {s: 'right' for s in mdp.S}\n",
    "\n",
    "# Generate some episodes.\n",
    "episodes = generate_episodes(mdp, policy, num_episodes=100000, max_steps=100)\n",
    "\n",
    "# Perform direct evaluation with these episodes.\n",
    "V = direct_evaluation(episodes, mdp.gamma)\n",
    "\n",
    "# Visualize values:\n",
    "# 1. Create grid for plotting.\n",
    "g = MDPGrid(n, n)\n",
    "axes = g.draw()\n",
    "# 2. Plot values with colors and numbers.\n",
    "g.plot_V(axes, V, print_numbers=True)\n",
    "# 3. Draw goal and obstacle cells\n",
    "g.draw_cell_circle(axes, goal, color='g')\n",
    "for ob in obstacles:\n",
    "    g.draw_cell_circle(axes, ob, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can convince yourself this is correct by running Policy Evaluation in the PSet 4 notebook with this always-go-right policy and comparing the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test your direct evaluation code here.\"\"\"\n",
    "Grader.run_single_test_inline(TestPSet5, \"test_1_direct_evaluation\", locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"sample_sensitivity\"></a> Sample Sensitivity (10 points)\n",
    "\n",
    "In class, we discussed that Direct Evaluation, while simple, has a number of limitations. We now want to observe its performance with varying sample sizes. Run the MDP example above with a varying number of samples; concretely, run the block with different values for `num_episodes` inside `generate_episodes`. Try 100, 1000, 10000, and 100000. For each `num_episodes` value run the block a few times, then answer the following:\n",
    "\n",
    "- What trends do you observe in the value estimates as you increase the number of episodes?\n",
    "- How does the number of samples affect the convergence of the value function?\n",
    "- Why does Direct Evaluation require a large number of samples to provide accurate estimates?\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Discuss your results in the cell below**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *(double click on this cell to delete this text and type your answer here)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"temporal_difference\"></a> Temporal Difference Learning (20 points)\n",
    "\n",
    "In this part of the problem, you will implement **Temporal Difference Learning**, which loops over the episode samples and performs the TD update to the value function one sample at a time. Your function should have the following signature:\n",
    "```python\n",
    "def td_learning(episodes, gamma, alpha)\n",
    "```\n",
    "\n",
    "The function takes in a list of episodes experienced by following the fixed policy in the MDP, the discount factor gamma, and the learning rate alpha for blending past values with the new sample. The function should return a Python dictionary with the estimated value for each state in the MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Implement the function `td_learning(episodes, gamma, alpha)` below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_learning(episodes, gamma, alpha):\n",
    "    \"\"\"\n",
    "    Temporal-Difference Learning to estimate the value function for a fixed policy.\n",
    "    \n",
    "    Args:\n",
    "        episodes (list): A list of episodes. Each episode is a list of (state, action, state', reward) tuples.\n",
    "        gamma (float): The discount factor of the MDP.\n",
    "        alpha (float): The learning rate.\n",
    "        \n",
    "    Returns:\n",
    "        V: A dictionary mapping states to their estimated value V(s).\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out how your code performs on the initial grid world we defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code for TD Learning with the example from before and a policy that always goes right.\n",
    "\n",
    "# Build MDP with p=0.8 and gamma=0.8. Use default rewards.\n",
    "n = 3\n",
    "goal = (2, 2)\n",
    "obstacles = [(0, 1)]\n",
    "mdp = build_mdp(n, 0.8, obstacles, goal, 0.8)\n",
    "\n",
    "# Create simple policy that always moves right.\n",
    "policy = {s: 'right' for s in mdp.S}\n",
    "\n",
    "# Generate some episodes.\n",
    "episodes = generate_episodes(mdp, policy, num_episodes=100000, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform TD learning with these episodes.\n",
    "alpha = 0.01\n",
    "V = td_learning(episodes, mdp.gamma, alpha)\n",
    "\n",
    "# Visualize values:\n",
    "# 1. Create grid for plotting.\n",
    "g = MDPGrid(n, n)\n",
    "axes = g.draw()\n",
    "# 2. Plot values with colors and numbers.\n",
    "g.plot_V(axes, V, print_numbers=True)\n",
    "# 3. Draw goal and obstacle cells\n",
    "g.draw_cell_circle(axes, goal, color='g')\n",
    "for ob in obstacles:\n",
    "    g.draw_cell_circle(axes, ob, color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test your TD learning code here.\"\"\"\n",
    "Grader.run_single_test_inline(TestPSet5, \"test_2_TD_learning\", locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"learning_rate\"></a> Learning Rate Sensitivity (10 points)\n",
    "\n",
    "In the code above, we ran TD Learning with alpha = 0.01. Try playing with more values of alpha: 0.00001, 0.0001, 0.001, 0.01, 0.1, 0.5. Answer the following based on what you observe:\n",
    "\n",
    "- How does a very low learning rate (e.g., alpha = 0.00001) affect the value function compared to a moderate learning rate (e.g., alpha = 0.01)? What about a very high learning rate (e.g., alpha = 0.5)?\n",
    "- Identify a learning rate that seems to work well for your specific MDP setup and explain why you think it strikes the right balance.\n",
    "- Reflecting on your results, how do you think the choice of learning rate might affect the performance of TD Learning in more complex environments?\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Discuss your results in the cell below**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *(double click on this cell to delete this text and type your answer here)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"problem2\"></a> Problem 2: Active Reinforcement Learning (35 points)\n",
    "\n",
    "Passive Reinforcement Learning is useful for evaluating fixed policies without explicit knowledge of T and R. However, it can't be used for turning these values into a new (improved) policy. For that, we need active interactions with the environment (not just passive reflection on past episodes). In this problem, we switch over to Active Reinforcement Learning, specifically to Q-Learning, and we will investigate the exploration-exploitation tradeoff and how that can affect the algorithm's efficiency.\n",
    "\n",
    "Since we are interacting with the world in real time now, instead of using `generate_episodes` to generate experiences to learn from, we will use a function `sample_environment` which given a state and an action it queries the MDP environment for the resulting next state and reward:\n",
    "\n",
    "```python\n",
    "def sample_environment(mdp, state, action)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try running the following block to see how to call `sample_environment` and what it returns. Try running it a few times (and possibly with different states and actions) to convince yourself that there is stochasticity involved. (Note if you runt the following cell after running code for Q-learning below you may find that it's no longer stochastic because we set a random seed in that part for grading.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build MDP with p=0.8 and gamma=0.8. Use default rewards.\n",
    "n = 3\n",
    "goal = (2, 2)\n",
    "obstacles = [(0, 1)]\n",
    "mdp = build_mdp(n, 0.8, obstacles, goal, 0.8)\n",
    "\n",
    "state = (1, 2)\n",
    "action = \"right\"\n",
    "next_state, reward = sample_environment(mdp, state, action)\n",
    "print(state, action, next_state, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into implementing Q-Learning, let's first see some Q-values. We will store Q-values as a Python dictionary that maps each state in the MDP to another dictionary that maps each action to the corresponding Q-value. To see an example, here's what a random Q-Value function looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A random Q-value function:\n",
    "def random_Q(states, actions):\n",
    "    # Initialize empty Q-values dictionary.\n",
    "    Q = {}\n",
    "    # Generate random Q-values between minV and maxV.\n",
    "    minQ, maxQ = -100, 100\n",
    "    for state in states:\n",
    "        Q[state] = {}\n",
    "        for action in actions:\n",
    "            Q[state][action] = np.random.uniform(minQ, maxQ)\n",
    "    return Q\n",
    "\n",
    "# Print values generated with the random Q-Value function.\n",
    "print(\"Q_random is a valid Q-Value function, although definitely not optimal (it's just random!)\")\n",
    "n = 3\n",
    "mdp = build_mdp(n, 0.8, [], (1, 1), 0.8)\n",
    "Q_random = random_Q(mdp.S, mdp.A)\n",
    "Q_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize these Q-values in the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid for plotting.\n",
    "g = MDPGrid(n, n)\n",
    "axes = g.draw()\n",
    "\n",
    "# Plot Q in the grid.\n",
    "g.plot_Q(axes, Q_random, print_numbers = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"Qlearning\"></a> Q-Learning (20 points)\n",
    "\n",
    "You will now implement **Q-Learning**. While Direct Evaluation and TD Learning were on-policy (they relied on samples coming from a fixed policy), Q-Learning is off-policy (doesn't depend on samples from one fixed policy), can choose its own actions as we go, and can converge to the optimal policy even if the agent is acting suboptimally. You should thus make use of `sample_environment` in your code to try out actions in the environment. For this problem, you should use a random action selection strategy meaning you should choose randomly from the available actions to sample the environment.\n",
    "\n",
    "Your function should have the following signature:\n",
    "```python\n",
    "def q_learning(num_episodes, gamma, alpha, max_steps=100)\n",
    "```\n",
    "\n",
    "The function takes in the number of episodes your agent can attempt in the MDP, the discount factor gamma, the learning rate alpha (similar to TD learning), and a maximum length for each episode (to avoid infinite episodes). The function should return a Python dictionary with the estimated Q-value for each (state, action) pair in the MDP, as well as a policy that results from maximizing the final values at each state.\n",
    "\n",
    "To help you out, we provide some starter code that initializes an empty Q-value dictionary, loops through episodes, and starts an episode at a random state. The function `mdp.is_sink_state(state)` checks whether a state is terminal or not; the episode ends right after we enter a terminal state or when we reach the maximum allowable length of the episode max_steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Implement the function `q_learning(num_episodes, gamma, alpha, max_steps=100)` below.\n",
    "\n",
    "Complete the two To dos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(num_episodes, mdp, gamma, alpha, max_steps=100):\n",
    "    \"\"\"\n",
    "    Active Q-Learning with random action selection.\n",
    "    \n",
    "    Args:\n",
    "        num_episodes (int): The number of episodes to run.\n",
    "        gamma (float): The discount factor of the MDP.\n",
    "        alpha (float): The learning rate.\n",
    "        max_steps (int): The maximum number of steps per episode to avoid infinite loops.\n",
    "        \n",
    "    Returns:\n",
    "        Q: A dictionary mapping states to another dictionary mapping actions to their estimated Q-values.\n",
    "        policy: A dictionary mapping states to actions to take.\n",
    "    \"\"\"\n",
    "    # in real life, you won't have access to mdp. We are including it as a parameter for testing purposes.\n",
    "    # DO NOT USE mdp.R or mdp.T!\n",
    "\n",
    "    # Initialize Q-value dictionary.\n",
    "    Q = {state: {action: 0.0 for action in mdp.A} for state in mdp.S}\n",
    "    \n",
    "    # Loop through episodes.\n",
    "    for episode in range(num_episodes):\n",
    "        # Start episode in a random state.\n",
    "        state = random.choice(list(mdp.S))\n",
    "        # Track number of steps in episode.\n",
    "        steps = 0\n",
    "        # While the episode is not done.\n",
    "        while not mdp.is_sink_state(state) and steps < max_steps:\n",
    "            steps += 1\n",
    "\n",
    "            # Pick an action randomly\n",
    "            action = random.choice(mdp.A)\n",
    "\n",
    "            ### TO DO 1: sample the environment, update Q-values using the Q-learning update rule, and transition to the next state.\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    ### TO DO 2: Derive policy from Q-values by maximizing over actions at each state.\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out how your code performs on the initial grid world we defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code for Q Learning with the example from before.\n",
    "\n",
    "# Build MDP with p=0.8 and gamma=0.8. Use default rewards.\n",
    "n = 3\n",
    "goal = (2, 2)\n",
    "obstacles = [(0, 1)]\n",
    "mdp = build_mdp(n, 0.8, obstacles, goal, 0.8)\n",
    "\n",
    "# Perform Q learning with these episodes.\n",
    "alpha = 0.01\n",
    "num_episodes = 10000\n",
    "Q, policy = q_learning(num_episodes, mdp, mdp.gamma, alpha)\n",
    "\n",
    "# Visualize values:\n",
    "# 1. Create grid for plotting.\n",
    "g = MDPGrid(n, n)\n",
    "axes = g.draw()\n",
    "# 2. Plot values with colors and numbers.\n",
    "g.plot_Q(axes, Q, print_numbers=True)\n",
    "# 3. Draw goal and obstacle cells\n",
    "g.draw_cell_circle(axes, goal, color='g')\n",
    "for ob in obstacles:\n",
    "    g.draw_cell_circle(axes, ob, color='k')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convince yourself that the optimal policy is the same one we got with Policy Extraction when we knew the ground truth T and R values in Problem Set 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the result.\n",
    "# 1. Create grid for plotting\n",
    "g = MDPGrid(n, n)\n",
    "axes = g.draw()\n",
    "# 2. Plot the values and the policy.\n",
    "g.plot_policy(axes, policy)\n",
    "# 3. Draw goal and obstacle cells\n",
    "g.draw_cell_circle(axes, goal, color='g')\n",
    "for ob in obstacles:\n",
    "    g.draw_cell_circle(axes, ob, color='k') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test your Q learning code here.\"\"\"\n",
    "# test_3_Q_learning(q_learning)\n",
    "Grader.run_single_test_inline(TestPSet5, \"test_3_Q_learning\", locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"epsilon_greedy\"></a> Epsilon-Greedy Q-Learning (10 points)\n",
    "\n",
    "We saw in class that an agent that selects actions randomly will take longer to explore the space and converge than one that has a more clever exploration strategy. We will now implement an **Epsilon-Greedy** variant of Q-Learning, meaning it chooses random actions an epsilon fraction of the time, and follows its current best Q-values otherwise. Note that choosing a random action may result in choosing the best action - that is, you should not choose a random sub-optimal action, but rather any random legal action.\n",
    "\n",
    "Your function should have the following signature:\n",
    "```python\n",
    "def q_learning_epsilon_greedy(num_episodes, gamma, alpha, epsilon, max_steps=100)\n",
    "```\n",
    "\n",
    "The function takes in the number of episodes your agent can attempt in the MDP, the discount factor gamma, the learning rate alpha, the epsilon exploration probability, and the maximum steps of an episode. The function should return a Python dictionary with the estimated Q-value for each state-action pair in the MDP, as well as a policy that results from maximizing the final values at each state.\n",
    "\n",
    "A good place to start is to just copy your code for `q_learning` below. \n",
    "\n",
    "**Important Notes**\n",
    "\n",
    "It's important to do the following for passing the test.\n",
    "- Please use `action = random.choice(mdp.A)` to generate the action for each step as we provided above in q-learning. \n",
    "- For the **Epsilon-Greedy**  part, please use `if random.uniform(0, 1) < epsilon:` then choose random action otherwise act on current policy.\n",
    "- You only need < 5 lines of change to your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Implement the function `q_learning_epsilon_greedy(num_episodes, gamma, alpha, epsilon, max_steps=100)` below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_epsilon_greedy(num_episodes, mdp, gamma, alpha, epsilon, max_steps=100):\n",
    "    \"\"\"\n",
    "    Active Q-Learning with epsilon-greedy action selection.\n",
    "    \n",
    "    Args:\n",
    "        num_episodes (int): The number of episodes to run.\n",
    "        gamma (float): The discount factor.\n",
    "        alpha (float): The learning rate.\n",
    "        epsilon (float): Exploration probability.\n",
    "        max_steps (int): The maximum number of steps per episode.\n",
    "        \n",
    "    Returns:\n",
    "        Q: A dictionary mapping states to another dictionary mapping actions to their estimated Q-values.\n",
    "        policy: A dictionary mapping states to the optimal actions.\n",
    "    \"\"\"\n",
    "    # in real life, you won't have access to mdp. We are including it as a parameter for testing purposes.\n",
    "    # BEGIN HERE!\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out how your code performs on the initial grid world we defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code for Epsilon Greedy Q Learning with the example from before.\n",
    "\n",
    "# Build MDP with p=0.8 and gamma=0.8. Use default rewards.\n",
    "n = 3\n",
    "goal = (2, 2)\n",
    "obstacles = [(0, 1)]\n",
    "mdp = build_mdp(n, 0.8, obstacles, goal, 0.8)\n",
    "\n",
    "# Perform Q learning with these episodes.\n",
    "alpha = 0.01\n",
    "epsilon = 0.5\n",
    "num_episodes = 10000\n",
    "Q, policy = q_learning_epsilon_greedy(num_episodes, mdp, mdp.gamma, alpha, epsilon)\n",
    "\n",
    "# Visualize values:\n",
    "# 1. Create grid for plotting.\n",
    "g = MDPGrid(n, n)\n",
    "axes = g.draw()\n",
    "# 2. Plot values with colors and numbers.\n",
    "g.plot_Q(axes, Q, print_numbers=True)\n",
    "# 3. Draw goal and obstacle cells\n",
    "g.draw_cell_circle(axes, goal, color='g')\n",
    "for ob in obstacles:\n",
    "    g.draw_cell_circle(axes, ob, color='k')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test your Epsilon Greedy Q learning code here.\"\"\"\n",
    "# test_4_Q_learning(q_learning_epsilon_greedy)\n",
    "Grader.run_single_test_inline(TestPSet5, \"test_4_Q_learning\", locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"exploration\"></a> Exploration vs. Exploitation (5 points)\n",
    "\n",
    "We will now run your code on the larger grid world with more obstacles. Execute the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "goal = (5,8)\n",
    "obstacles = [(1,3), (9,0), (8,8)] + \\\n",
    "            [(4, 2), (4, 3), (4, 6)] + \\\n",
    "            [(6, 2), (6, 3), (6, 5), (6, 6)]\n",
    "mdp = build_mdp(n, p=0.8, obstacles=obstacles, goal=goal, gamma=0.8, goal_reward=100, obstacle_reward=-500)\n",
    "\n",
    "# Perform Q learning with these episodes.\n",
    "alpha = 0.01\n",
    "epsilon = 0.1\n",
    "num_episodes = 50000\n",
    "Q, policy = q_learning_epsilon_greedy(num_episodes, mdp, mdp.gamma, alpha, epsilon)\n",
    "\n",
    "# Visualize values:\n",
    "# 1. Create grid for plotting.\n",
    "g = MDPGrid(n, n)\n",
    "axes = g.draw()\n",
    "# 2. Plot values with colors and numbers.\n",
    "g.plot_Q(axes, Q, print_numbers=False)\n",
    "g.plot_policy(axes, policy)\n",
    "# 3. Draw goal and obstacle cells\n",
    "g.draw_cell_circle(axes, goal, color='g')\n",
    "for ob in obstacles:\n",
    "    g.draw_cell_circle(axes, ob, color='k')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try running the same code block with different values of epsilon: 0 (pure exploitation), 0.1, 0.5, 1.0 (pure exploration). Try also varying num_episodes to get an idea of how many samples the algorithm needs to converge with different epsilon values. Answer the following questions:\n",
    "\n",
    "- How does changing the epsilon parameter affect the performance and policy learned by the agent?\n",
    "- Can you find a balance between exploration and exploitation that leads to both fast convergence and a good policy?\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Discuss your results in the cell below**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *(double click on this cell to delete this text and type your answer here)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"part4\"></a> Time Spent on Pset (5 points)\n",
    "\n",
    "Please use [this form](https://forms.gle/iRvW9zKmmJ8eFiPX7) to tell us how long you spent on this pset. After you submit the form, the form will give you a confirmation word. Please enter that confirmation word below to get an extra 5 points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_confirmation_word = \"\" #\"ENTER THE CONFIRMATION WORD HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all tests\n",
    "Grader.grade_output([TestPSet5], [locals()], \"results.json\")\n",
    "Grader.print_test_results(\"results.json\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
